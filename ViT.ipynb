{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajsal-ali/vit-from-scratch/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT) from Scratch\n",
        "\n",
        "In this notebook, we will implement a Vision Transformer (ViT) using PyTorch. The ViT model applies the transformer architecture—originally designed for NLP—to image classification tasks by treating image patches as tokens. We'll go through building patch embeddings, multi-head self-attention, transformer blocks, and finally assembling the complete ViT for classifying images.\n"
      ],
      "metadata": {
        "id": "_h0sxD4mxkbr"
      },
      "id": "_h0sxD4mxkbr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018f692d",
      "metadata": {
        "id": "018f692d"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7_JIP0L917dL"
      },
      "id": "7_JIP0L917dL"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "69d524e4",
      "metadata": {
        "id": "69d524e4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3583b9d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3583b9d2",
        "outputId": "9f584d23-9d85-4691-be53-595f188a034c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT Architecture Components\n",
        "\n",
        "Below is the full implementation of a Vision Transformer (ViT) broken down into modular PyTorch classes:\n",
        "\n",
        "- **PatchEmbedding**: Converts an image into a sequence of flattened patches and projects them into a lower-dimensional embedding space. It uses unfolding to extract patches and a linear layer to embed them.\n",
        "\n",
        "- **Head & MultiHeadAttention**: Implements scaled dot-product attention for each head, then concatenates their outputs. Each head computes attention scores between patches and aggregates patch information.\n",
        "\n",
        "- **FeedForward**: A standard MLP with a hidden layer (4× expansion) and ReLU activation used after attention for richer representations.\n",
        "\n",
        "- **Block**: Represents one transformer layer. It contains multi-head self-attention and a feedforward network, both wrapped with residual connections and layer normalization.\n",
        "\n",
        "- **VisionTransformer**: The main ViT class. It includes a learnable class token, positional embeddings, and a sequence of transformer blocks. After processing, it extracts the class token's output for final classification via a linear layer.\n",
        "\n",
        "This architecture is compatible with image sizes like 32×32 and can be trained on datasets like CIFAR-10.\n"
      ],
      "metadata": {
        "id": "ViQkaQS1xt-e"
      },
      "id": "ViQkaQS1xt-e"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "35ed331e",
      "metadata": {
        "id": "35ed331e"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self,image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_size= image_size\n",
        "        self.patch_size=patch_size\n",
        "        self.in_channels=in_channels\n",
        "        self.embed_dim=embed_dim\n",
        "        self.patch_dim=in_channels*patch_size*patch_size\n",
        "        num_patches=(self.image_size//self.patch_size)**2\n",
        "        self.proj=nn.Linear(self.patch_dim,self.embed_dim)\n",
        "    def forward(self,x):\n",
        "        B,C,H,W=x.shape\n",
        "        # print(f\"Input shape: {x.shape}\")\n",
        "        x=x.unfold(2,self.patch_size,self.patch_size).unfold(3,self.patch_size,self.patch_size)\n",
        "        # print(f\"Unfolded shape: {x.shape}\")\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5)\n",
        "        # print(f\"Permuted shape: {x.shape}\")\n",
        "        x=x.contiguous().view(B,-1,self.patch_dim)\n",
        "        # print(f\"Reshaped shape: {x.shape}\")\n",
        "        x=self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size,n_embd, block_size, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        head_size = k.size(-1)\n",
        "        wei = q @ k.transpose(-2,-1) * head_size**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size , n_embd, block_size, dropout=0.0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout=0.0):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size , n_embd, block_size, dropout)\n",
        "        self.ffwd = FeedFoward(n_embd,dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size=32, patch_size=8, num_classes=10,\n",
        "                 dim=128, depth=4, heads=4, mlp_dim=256, channels=3, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size * patch_size\n",
        "        self.patch_embedding = PatchEmbedding(image_size, patch_size, channels, dim)\n",
        "        self.class_embedding = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.possition_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.blocks= nn.Sequential(*[Block(dim, heads, num_patches + 1, dropout) for _ in range(depth)])\n",
        "        self.to_class_token = nn.Identity()\n",
        "        self.ln= nn.LayerNorm(dim)\n",
        "        self.linear = nn.Linear(dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x= self.patch_embedding(x)\n",
        "        cls_token = self.class_embedding.expand(B, -1, -1)\n",
        "        x=torch.cat((cls_token,x),dim=1)\n",
        "        x+= self.possition_embedding\n",
        "        x=self.blocks(x)\n",
        "        x=self.ln(x[:, 0])\n",
        "        x=self.linear(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation: CIFAR-10\n",
        "\n",
        "We load the CIFAR-10 dataset using `torchvision`. Basic transformations are applied to convert the images to tensors. Although no data augmentation or normalization is used here, these can be easily added for improved performance.\n",
        "\n",
        "- **Train/Test Split**: CIFAR-10 is split into training and test sets.\n",
        "- **Transformations**: Currently only `ToTensor()` is applied, which scales pixel values to [0, 1].\n",
        "- **Data Loaders**: Batches are created for training and testing with a batch size of 1024.\n"
      ],
      "metadata": {
        "id": "IgDjj3F-x_Ep"
      },
      "id": "IgDjj3F-x_Ep"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb09d5ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb09d5ca",
        "outputId": "d416ba59-614b-4b45-89c3-f6144c56a7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 31.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 49, Test batches: 10\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "# Define transformations for the training and test sets\n",
        "train_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    # Normally, you'd add normalization and perhaps random flips/crops here for augmentation\n",
        "])\n",
        "test_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    # Corresponding normalization (using same mean/std as train if applied)\n",
        "])\n",
        "# Download and load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=test_transform, download=True)\n",
        "# Create data loaders for batching\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
        "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Vision Transformer\n",
        "\n",
        "We initialize the ViT model with defined hyperparameters and train it on CIFAR-10 for 200 epochs using the Adam optimizer and cross-entropy loss. The model is trained on GPU if available.\n"
      ],
      "metadata": {
        "id": "Ju10CZ-GyRvS"
      },
      "id": "Ju10CZ-GyRvS"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "68642763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68642763",
        "outputId": "f996ce68-67bc-4f0a-9ec8-dd8f47e10abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.2969\n",
            "Epoch [2/20], Loss: 0.2848\n",
            "Epoch [3/20], Loss: 0.2836\n",
            "Epoch [4/20], Loss: 0.2870\n",
            "Epoch [5/20], Loss: 0.2821\n",
            "Epoch [6/20], Loss: 0.2771\n",
            "Epoch [7/20], Loss: 0.2789\n",
            "Epoch [8/20], Loss: 0.2752\n",
            "Epoch [9/20], Loss: 0.2772\n",
            "Epoch [10/20], Loss: 0.2727\n",
            "Epoch [11/20], Loss: 0.2730\n",
            "Epoch [12/20], Loss: 0.2705\n",
            "Epoch [13/20], Loss: 0.2713\n",
            "Epoch [14/20], Loss: 0.2719\n",
            "Epoch [15/20], Loss: 0.2667\n",
            "Epoch [16/20], Loss: 0.2670\n",
            "Epoch [17/20], Loss: 0.2666\n",
            "Epoch [18/20], Loss: 0.2597\n",
            "Epoch [19/20], Loss: 0.2561\n",
            "Epoch [20/20], Loss: 0.2627\n"
          ]
        }
      ],
      "source": [
        "# Move model to device (GPU if available)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "model = VisionTransformer(image_size=32, patch_size=4, num_classes=10, dim=128, depth=4, heads=4, mlp_dim=256)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)            # forward pass\n",
        "        loss = criterion(outputs, labels)  # compute loss\n",
        "        loss.backward()                    # backpropagate gradients\n",
        "        optimizer.step()                   # update parameters\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "The trained Vision Transformer is evaluated on the test set without gradient calculations. Accuracy is computed by comparing predicted and true labels.\n"
      ],
      "metadata": {
        "id": "ckEZzgwkyXyQ"
      },
      "id": "ckEZzgwkyXyQ"
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():  # no gradient needed for eval\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUt39BnVQWcz",
        "outputId": "0a34e4f1-0f64-4c74-ee90-8fd479682fb3"
      },
      "id": "yUt39BnVQWcz",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 62.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4R0r2UqScPe"
      },
      "id": "o4R0r2UqScPe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}